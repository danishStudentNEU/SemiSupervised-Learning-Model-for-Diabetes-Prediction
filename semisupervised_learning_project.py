# -*- coding: utf-8 -*-
"""Semi-Supervised Learning Pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LTaCu0PjUHXV9ibIxHhoFX3xM2rbt6K6
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score
import warnings


warnings.filterwarnings('ignore')

def load_data(filepath):
    """Loads data, handling missing values."""
    return pd.read_csv(filepath, na_values=['', ' ', 'NA'])

# STEP 1: Data Pre-processing Phase
def preprocess_data(df, target_column=None, is_employee_data=False):
    """
    Performs encoding, missing value , outlier removal and normalization.
    """
    print(f"\n--- Pre-processing Data ({'Employee' if is_employee_data else 'Diabetes'}) ---")
    data = df.copy()
    # this is before imputation to handle strings
    if is_employee_data:
        le = LabelEncoder()
        for col in data.select_dtypes(include=['object']).columns:
            # Handle NaNs in object columns before encoding
            data[col] = data[col].fillna(data[col].mode()[0])
            data[col] = le.fit_transform(data[col])
            print(f"Encoded column: {col}")

    # 2. Impute missing values
    # Using Mean
    imputer = SimpleImputer(strategy='mean')
    data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)

    # 3. Remove Outliers (IQR Method)
    cols_to_check = [c for c in data_imputed.columns if c != target_column]

    Q1 = data_imputed[cols_to_check].quantile(0.25)
    Q3 = data_imputed[cols_to_check].quantile(0.75)
    IQR = Q3 - Q1

    # keeping rows that are not outliers
    # (Values must be between Q1 - 1.5*IQR and Q3 + 1.5*IQR)
    mask = ~((data_imputed[cols_to_check] < (Q1 - 1.5 * IQR)) | (data_imputed[cols_to_check] > (Q3 + 1.5 * IQR))).any(axis=1)
    data_clean = data_imputed[mask]

    print(f"Rows before outlier removal: {len(data_imputed)}")
    print(f"Rows after outlier removal: {len(data_clean)}")

    if target_column and target_column in data_clean.columns:
        y = data_clean[target_column]
        X = data_clean.drop(columns=[target_column])
    else:
        y = None
        X = data_clean

    # 4. Normalize all columns
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

    return X_scaled, y

# STEP 2: Unsupervised Learning for Labels
def generate_labels_diabetes(X_scaled, original_columns):
    """
    Specific to Diabetes Dataset:
    Clusters based on Glucose, BMI, Age to create 'Outcome'.
    """
    print("\n--- Step 2: Unsupervised Learning (Clustering) ---")
    cluster_features = ['Glucose', 'BMI', 'Age']
    X_cluster = X_scaled[cluster_features]

    # K-Means Clustering
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_cluster)

    # Determine which cluster is 'Diabetes'
    # Logic: Cluster with higher average Glucose = Diabetes1

    # Add temporary cluster label to calculate means
    X_cluster_temp = X_cluster.copy()
    X_cluster_temp['cluster'] = clusters

    # Calculate mean Glucose per cluster and assign labels: 1 if cluster == diabetes_cluster_id, else 0
    means = X_cluster_temp.groupby('cluster')['Glucose'].mean()
    print(f"Cluster Glucose Means: \n{means}")
    diabetes_cluster_id = means.idxmax()
    labels = np.where(clusters == diabetes_cluster_id, 1, 0)

    print(f"Labels assigned. Class distribution: {np.bincount(labels)}")
    return labels

# STEP 3: Feature Extraction (PCA)
def perform_pca(X_train, X_test, n_components=3):
    """
    Use PCA to reduce dimensionality.
    """
    print("\n--- Step 3: Feature Extraction (PCA) ---")

    pca = PCA(n_components=n_components)

    # Fit on training, transform both
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    print(f"explained variance ratio: {pca.explained_variance_ratio_}")

    return X_train_pca, X_test_pca

# STEP 4: Classification (Super Learner)
def train_super_learner(X_train, y_train, X_test, y_test):
    """
    Trains a Stacking Classifier with NB, MLP, KNN as base
    and Decision Tree as meta-learner. Performs Hyperparameter tuning.
    """
    print("\n--- Step 4: Classification (Super Learner / Stacking) ---")

    # 1. Define Base Classifiers
    base_learners = [
        ('nb', GaussianNB()),
        ('mlp', MLPClassifier(max_iter=500, random_state=42)),
        ('knn', KNeighborsClassifier())
    ]

    # 2. Define Meta Learner
    meta_learner = DecisionTreeClassifier(random_state=42)

    # 3. Define Super Learner (Stacking)
    clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=5)

    # 4. Hyperparameter Tuning
    # We tune parameters for both base learners and the meta learner
    # Tuning MLP in grid search is slow, so we keep the grid small for demonstration
    param_grid = {
        'knn__n_neighbors': [3, 5, 7],
        'final_estimator__max_depth': [3, 5, 10],
        'final_estimator__criterion': ['gini', 'entropy']
    }

    print("Starting Grid Search for Hyperparameters...")
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    grid.fit(X_train, y_train)

    print(f"Best Parameters: {grid.best_params_}")

    # 5. Report
    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    print(f"Best Model Accuracy on Test Data: {acc:.4f}")
    return best_model

# MAIN EXECUTION FLOW
def main():

    # PART A: DIABETES DATASET (Steps 1-4)
    print("="*40)
    print("PROCESSING DIABETES DATASET")
    print("="*40)

    try
        df_diabetes = load_data('diabetes_project.csv')

        # Step 1: Pre-process
        X_diabetes_scaled, _ = preprocess_data(df_diabetes, target_column=None)

        # Step 2: Generate Labels
        y_diabetes_generated = generate_labels_diabetes(X_diabetes_scaled, df_diabetes.columns)

        # Step 3: Feature Extraction
        # Split data 80/20
        X_train, X_test, y_train, y_test = train_test_split(
            X_diabetes_scaled, y_diabetes_generated, test_size=0.2, random_state=42
        )

        # Apply PCA
        X_train_pca, X_test_pca = perform_pca(X_train, X_test)

        # Step 4: Classification
        train_super_learner(X_train_pca, y_train, X_test_pca, y_test)

    except FileNotFoundError:
        print("Error: diabetes_project.csv not found.")
    except Exception as e:
        print(f"An error occurred during Diabetes processing: {e}")

    # PART B: EMPLOYEE DATASET (Step 5)
    print("\n" + "="*40)
    print("PROCESSING EMPLOYEE DATASET (Step 5)")
    print("="*40)

    try:
        # Load
        df_employee = load_data('Employee_Leave.csv')

        # Use the last column as outcome
        target_col_name = df_employee.columns[-1]
        print(f"Target Column identified as: {target_col_name}")

        # Reuse Step 1: Pre-process With modifications for Encoding
        # We pass target(last)_col so it isn't used in outlier detection logic for feature but handled separately
        X_emp_scaled, y_emp = preprocess_data(df_employee, target_column=target_col_name, is_employee_data=True)

        # Skip Step 2 (Labels already exist)

        # Reuse Step 3: Feature Extraction
        X_train_emp, X_test_emp, y_train_emp, y_test_emp = train_test_split(
            X_emp_scaled, y_emp, test_size=0.2, random_state=42
        )

        # PCA
        X_train_emp_pca, X_test_emp_pca = perform_pca(X_train_emp, X_test_emp)

        # Reuse Step 4: Classification
        train_super_learner(X_train_emp_pca, y_train_emp, X_test_emp_pca, y_test_emp)

    except FileNotFoundError:
        print("Error: Employee_Leave.csv not found.")
    except Exception as e:
        print(f"An error occurred during Employee processing: {e}")

if __name__ == "__main__":
    main()